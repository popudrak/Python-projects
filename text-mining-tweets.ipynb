{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projekt kod",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesyraf/-Text-mining-based-on-tweets-of-US-presidential-candidates/blob/main/projekt_kod.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghdpTsMFOr_9"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk as nltk\n",
        "%pip install nltk\n",
        "%pip install re\n",
        "%pip install gensin\n",
        "%pip install seaborn\n",
        "nltk.download('stopwords')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaP8EvOgQgK0"
      },
      "source": [
        "%pip install pickle5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9OzmUQZYxJS"
      },
      "source": [
        "import pickle5 as pickle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYQdbjWvGKzq"
      },
      "source": [
        "tweets_text = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvDwPFtHXIQQ"
      },
      "source": [
        "filename = \"Election_tweets.pkl\"\n",
        "\n",
        "with open(filename, \"rb\") as file:\n",
        "  while True:\n",
        "    try:\n",
        "      tweet = pickle.load(file)\n",
        "      tweets_text.append(tweet['full_text'])\n",
        "    except EOFError:\n",
        "      break\n",
        "\n",
        "df = pd.DataFrame(text, columns=[\"Tweets\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1bai9wTJoUf"
      },
      "source": [
        "\"MrTrumpSpeeches.csv\".encode('utf-8').strip()\n",
        "\"joe_biden_dnc_2020.csv\".encode('utf-8').strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dml8lAMlOzBI"
      },
      "source": [
        "Trump = pd.read_csv(\"MrTrumpSpeeches.csv\", encoding= 'latin-1', sep=\"~\")\n",
        "Biden = pd.read_csv(\"joe_biden_dnc_2020.csv\", encoding= 'latin-1', sep=\"~\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiqSW7u7O0bV"
      },
      "source": [
        "Trump_string = \" \".join(Trump['subtitles'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JiTRpAIPS21"
      },
      "source": [
        "Biden_string = \" \".join(Biden['TEXT'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bA82AhnEPeVu"
      },
      "source": [
        "PREPROCESING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFGTCV57Pf_B"
      },
      "source": [
        "#removal of stopwords\n",
        "#removal of special characters (\"emoticons\", punctuation)\n",
        "#removal of numbers\n",
        "#removal of duplicates\n",
        "#tokenization\n",
        "#stemantization, lemantization\n",
        "#finding bigrames and trigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXQB-L-Jfoj0"
      },
      "source": [
        "Basic stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jG1X8QXZYuz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7ee689c-4b02-4323-e5f7-2cbcb2f2735d"
      },
      "source": [
        "#lista stopwordsów\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siuuRRGqZ6P8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59d47fff-4e1c-4d67-f426-04cef9724fc4"
      },
      "source": [
        "len(Trump_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etapwJcraoQU"
      },
      "source": [
        "#tokenization\n",
        "Trump_string.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNI7p58MbjYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a637ada4-de38-4beb-df4e-aa65a6509110"
      },
      "source": [
        "Trump_string_split = Trump_string.split()\n",
        "len([word for word in Trump_string_split if word in stop_words])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1350439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLdPXogKdGF3"
      },
      "source": [
        "#number of stopwords\n",
        "Trump['liczba_stopwords'] = Trump['subtitles'].apply(lambda x: len ([word for word in x.split() if word in stop_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0bsPQMqxvIR"
      },
      "source": [
        "Trump['zmalych'] = Trump['subtitles'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJae3eA1x9a3"
      },
      "source": [
        "Trump['stopwords'] = Trump['zmalych'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJfjduov8oF"
      },
      "source": [
        "Trump['tokeny'] = Trump['stopwords'].apply(lambda x: x.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rvsE1-7ik0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f10da2f-021f-43b3-a360-0411cc8d751d"
      },
      "source": [
        "#number of numbers\n",
        "len([word for word in Trump_string if word.isdigit])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KttmQg-viiCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3da2d79f-6e8e-4422-aa1c-7dc07fc23a45"
      },
      "source": [
        "#number of words\n",
        "len(Trump_string.split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3574984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2SFJbEli9nv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607120b6-5b60-4591-d535-74797f47ba28"
      },
      "source": [
        "#number of characters that contain spaces\n",
        "len(Trump_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15561807"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqyRjNY2cKza",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53dd8c0-7f6f-443b-a211-f842771fa049"
      },
      "source": [
        "#number of words written in capital letters\n",
        "len([word for word in Trump_string.split(\" \") if word.isupper])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3574984"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A0a3UDNj33K"
      },
      "source": [
        "słowa_Trumpa_z_dużych_liter = ([word for word in Trump_string.split(\" \") if word.isupper])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXaVhEBokDyz"
      },
      "source": [
        "#number of \"I\"\n",
        "len(for word in słowa_Trumpa_z_dużych_liter):\n",
        "  if \"I\" in słowa_Trumpa_z_dużych_liter:\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejuG_P-wnn5h"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1DDr4WUnq-Y"
      },
      "source": [
        "#change to lowercase\n",
        "data = Trump_string.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBYyDj8od1h"
      },
      "source": [
        "#removal of punctuation\n",
        "#regexr.com\n",
        "data = re.sub('[^\\w\\s], \"\", data')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PEIuI0rp1ml"
      },
      "source": [
        "#number of numbers in data\n",
        "len([word for word in data if word.isdigit{}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GirYXzwSqNRd"
      },
      "source": [
        "#removal of numbers\n",
        "data = re.sub('\\d+',\"\", data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx4MqRmbrJRS"
      },
      "source": [
        "#tokenization before stopwords removal\n",
        "tokenized_words = data.split() #splits into words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUFT7oCJreKv"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "tkoenized_word_1 = word_tokenized(data) #different approach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qp9mF4-r46S"
      },
      "source": [
        "#removal of stopwords\n",
        "filtered_word = []\n",
        "for word in tokenized_words:\n",
        "    if word not in stop_words:\n",
        "        filtered_word.append(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_KYy3Q2slJ0"
      },
      "source": [
        "filtered_word #words without stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgokf438tjli"
      },
      "source": [
        "Word frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gEYz1KdtslY"
      },
      "source": [
        "fdist = FreqDist(filtered_word)\n",
        "common = fdist.most_common(20) #top20 most common"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQs6k0q4Wnlk"
      },
      "source": [
        "tablica=pd.DataFrame(common, columns=['words','count'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6i-y961XpgM"
      },
      "source": [
        "#seaborn plots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Q_oINFYW7B"
      },
      "source": [
        "fig, as = plt.subplots(figsize=(10,10))\n",
        "colors = sns.color_palette (\"husl\",10)\n",
        "tablica.sort_values(by='count').plot.barh(x='words',y='count', ax=ax, color = colors, alpha=0,5)\n",
        "\n",
        "ax.set.title('najczęściejwystępujące słowa w wypowiedziach Trumpa')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfuqIs0jZ1mb"
      },
      "source": [
        "Word clouds\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4HCQ1q_Z5IT"
      },
      "source": [
        " pip install wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVqVkJQWZ8uC"
      },
      "source": [
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "from matplotlib import cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v_2quJVaIyI"
      },
      "source": [
        "color = cm.Greens\n",
        "\n",
        "wc = WordCloud(background_color ='white', colormap = color, \n",
        "               max_words = 500, #max_font_size = 20, width=2000, height=1500)\n",
        "\n",
        "wc.generate(data)\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis('off');  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lQwzlv8cBUz"
      },
      "source": [
        "pip install PIL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxMK4nFvcDbW"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvQSGtedMto"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr9MyvSAcL6q"
      },
      "source": [
        "mask = np.array(Image.open('jakieśzdjęcie.png'))\n",
        "imagine_colors = ImagineColorGenerator(mask)\n",
        "\n",
        "wc = WordCloud(background_color ='white', mask = mask, max_words = 50000, contour_width = 0.01)\n",
        "my_wordcloud = wc.generate(data)\n",
        "\n",
        "plt.figure(figsize=[10,10])\n",
        "plt.imshow(my_wordcloud.recolor(color_func = image_colors), interpolation='bilinear')\n",
        "plt.axis('off');\n",
        "plt.savefig(\"wordcloud.png\", format='png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpUhGLSedwaL"
      },
      "source": [
        "Bigrams and trigrams (double and triple words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQQoTEsdywr"
      },
      "source": [
        "pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZX2ExrRNd2wn"
      },
      "source": [
        "pip install collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LoHf3Y1d7W9"
      },
      "source": [
        "import collections\n",
        "from nltk import bigrams, trigrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzf8K4JFeHJr"
      },
      "source": [
        "bigramy = list(bigrams(filtered_word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP8rQFLmepwC"
      },
      "source": [
        "trigramy = list(trigrams(filtered_word))\n",
        "frequency_trigram = collections.Counter(trigramy)\n",
        "common_trigram = frequency.most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC6fUMWze7-p"
      },
      "source": [
        "Stematization and lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2mW4pn_e_KY"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CViUGc4fThI"
      },
      "source": [
        "ps = PortStemmer ()\n",
        "\n",
        "stemmed_words = []\n",
        "\n",
        "for word in filtered_word:\n",
        "  stemmed_words.append(ps.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPwYhGl1gVYS"
      },
      "source": [
        "lem = WordNetLemmatizer ()\n",
        "\n",
        "lemmed_words = []\n",
        "\n",
        "for word in filtered_word:\n",
        "  lemmed_words.append(lem.lemmatize(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmZHeEM1fct6"
      },
      "source": [
        "filtered_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of0s8e2yg2Vv"
      },
      "source": [
        "Preprocessing DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzYZ7L5Rg7kC"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAZioh4RhBK9"
      },
      "source": [
        "Trump = pd.read_csv(\"MrTrumpSpeeches.csv\", sep=\"~\", encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_IWqHGWPXUk"
      },
      "source": [
        "Biden=pd.read_csv(\"joe_biden_dnc_2020.csv\", sep=\"~\", encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Wb_xHahQhJ"
      },
      "source": [
        "Trump['filtered_words'] = Trump['subtitles'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH_kMvf5PyKr"
      },
      "source": [
        "Biden['filtered_words1']= Biden['TEXT'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3_Zx_TehgaJ"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: re.sub('\\d+', \"\", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1zQMVyyQUBf"
      },
      "source": [
        "Biden['filtered_words1']=Biden['filtered_words1'].apply(lambda x: re.sub('\\d+', \"\",x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwJGMfhTh521"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: re.sub('[^\\\\w\\s]', \"\", x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anmP4PkuRRhV"
      },
      "source": [
        "Biden['filtered_words1']= Biden['filtered_words1'].apply (lambda x:re.sub ('[^\\\\w\\s]',\"\",x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlZ7wKfWiHt0"
      },
      "source": [
        "def remove_my_stopwords(text):\n",
        "  stop_words = stopwords.words(\"english\")\n",
        "  stop_words.extend(['la', '[music]'])\n",
        "  filtered_word = []\n",
        "  for word in text.split():\n",
        "    if word not in stop_words:\n",
        "      filtered_word.append(word)\n",
        "  return filtered_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRJ1nTvMR_fq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgBw_a4Fivqo"
      },
      "source": [
        "Trump['filtered_words'] = Trump['filtered_words'].apply(lambda x: remove_my_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dwKjLE_jG0R"
      },
      "source": [
        "Term frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd8nmwEGjI4z"
      },
      "source": [
        "dane = []\n",
        "for i in range(len(Trump)):\n",
        "    for j in Trump['filtered_words'][i]:\n",
        "        dane.append(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyKX2fQgSUxL"
      },
      "source": [
        "dane1=[]\n",
        "for i in range (len(Biden)):\n",
        "  for j in Biden ['filtered_words1'][i]:\n",
        "    dane1.append (j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfy7BTm2jW8j"
      },
      "source": [
        "df1 = pd.DataFrame(dane, columns=['words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut18PnqySmsY"
      },
      "source": [
        "df2=pd.DataFrame(dane1, columns=['words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NqmizcAjfaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a359528f-541b-4584-d426-0752148f7b09"
      },
      "source": [
        "round(df1.words.value_counts()/len(df1),4)*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "going          1.97\n",
              "people         1.38\n",
              "know           1.21\n",
              "dont           0.80\n",
              "great          0.78\n",
              "               ... \n",
              "antagonist     0.00\n",
              "fairgrounds    0.00\n",
              "stroking       0.00\n",
              "patchy         0.00\n",
              "rauf           0.00\n",
              "Name: words, Length: 23067, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th5ceq1XSwnD"
      },
      "source": [
        "round(df2.words.value_counts()/len(df2),4)*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c4osa2Cjrp8"
      },
      "source": [
        "Document Term Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzGPOBujutP"
      },
      "source": [
        "most_frequent1000 = df1.words.value_counts().head(1000).keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq56DqItTTa0"
      },
      "source": [
        "most_frequent1000=df2.words.value_counts().head(1000).keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TcMSAppj5Le"
      },
      "source": [
        "freq = pd.DataFrame(index=most_frequent1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXgRGJJ1TezL"
      },
      "source": [
        "freq= pd.DataFrame(index=most_frequent1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTPEJTVEjjTj"
      },
      "source": [
        "for index,row in Trump.iterrows(): #loop Trump\n",
        "    frequences = [] \n",
        "    for word in most_frequent1000:\n",
        "        frequences.append(row.filtered_words.count(word)) \n",
        "       \n",
        "    freq[f\"(row.title)\"] = frequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pv--Ia_Tm7I"
      },
      "source": [
        "for index,row in Biden.iterrows(): #loop Biden\n",
        "    frequences = []\n",
        "    for word in most_frequent1000:\n",
        "        frequences.append(row.filtered_words1.count(word)) \n",
        "\n",
        "    freq[f\"(row.title)\"] = frequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bF4jQrIUZTG"
      },
      "source": [
        "freq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA73VAQPkhnn"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(20,20))\n",
        "\n",
        "ax.set_title(\"Częstość występowania słów w wypowiedziach Trumpa\", fontsize=20)\n",
        "wykres = sns.heatmap(freq, vmin=0, vmax=50, cmap=\"GnBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgVSIZcwk_Mp"
      },
      "source": [
        "Hierarchical, agglomerative grouping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP39HQxolB3W"
      },
      "source": [
        "pip install scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVxYAeizlEg-"
      },
      "source": [
        "from scipy.cluster.hierarchy import cophenet, dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aHzqmTEWfKq"
      },
      "source": [
        "freq = (freq.T).head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHWsBTHslP5A"
      },
      "source": [
        "Y = pdist(freq,'cosine');\n",
        "linked=linkage(Y,'complete');\n",
        "plt.figure(figsize=(20, 10))\n",
        "labels=freq.index\n",
        "labels=labels.to_list()\n",
        "dendrogram(linked, labels=labels)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnTos38k0iKX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
